{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All initialization tests passed.\n",
      "imported herringbone without any errors :)\n"
     ]
    }
   ],
   "source": [
    "import herringbone as hb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an MDP\n",
    "\n",
    "An MDP is formally defined as a 5-tuple $\\mathcal{M} = (S, A, P, R, \\gamma)$, where:\n",
    "- $S$ defines the state space\n",
    "- $A$ defines the action space\n",
    "- $P$ models the environment dynamics\n",
    "- $R$ models the reward function\n",
    "- $\\gamma$ defines the discount factor\n",
    "\n",
    "To create an MDP with this framework, it needs paths to at least a state config, a map, and an action config.\n",
    "\n",
    "Additionally, it can take an array of transition matrices (see $P$ in formal MDP definition), a seed, and the discount factor $\\gamma$. \n",
    "But these have default values, so do not fret if you do not understand them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_names = [\"slides\", \"example\", \"easy\", \"danger_holes\", \"double_fish\", \"wall_of_death\", \"example2\", \"mega\"]\n",
    "selected_map_id = 2\n",
    "\n",
    "state_path = \"herringbone/env_core/config/state_config.json\"\n",
    "map_path = f\"herringbone/env_core/maps/{map_names[selected_map_id]}.csv\"\n",
    "action_path = \"herringbone/env_core/config/action_config.json\"\n",
    "\n",
    "gamma = 1\n",
    "\n",
    "mdp = hb.MDP(state_path, map_path, action_path, seed=42, gamma=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previewing the board\n",
    "\n",
    "The board can be previewed with the following code.\n",
    "\n",
    "**Render Modes**\n",
    "1. `'sar'`: prints the state, action, reward of each iteration (only used in Monte Carlo simulations and Temporal Difference learning)\n",
    "2. `'rewards'`: prints the board with the calculated rewards for each state\n",
    "3. `'ascii'`: prints an ascii representation of the board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔═════════╦═════════╦═════════╗\n",
      "║ \u001b[34m       \u001b[0m ║ \u001b[34m       \u001b[0m ║ \u001b[32m<x)))><\u001b[0m ║\n",
      "╚═════════╩═════════╩═════════╝[0]\n"
     ]
    }
   ],
   "source": [
    "render_modes = ['sar', 'rewards', 'ascii']\n",
    "hb.Render.preview_frame(board=mdp.get_board(), agent_state=None, render_mode=render_modes[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a policy & running an episode\n",
    "\n",
    "A policy can be created with help of de MDP, this policy is unfirom/random by default.\n",
    "\n",
    "An episode is created with an MPD and a policy, and a max depth to ensure that it does not run forever with a sub optimal policy.\n",
    "\n",
    "This episode instance can be ran with a render method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔═══════╦═══════╦═══════╗\n",
      "║ \u001b[34m  -1 \u001b[0m ║ \u001b[34m  -1 \u001b[0m ║ \u001b[32m  10 \u001b[0m ║\n",
      "╚═══════╩═══════╩═══════╝[0]\n",
      "╔═════════╦═════════╦═════════╗\n",
      "║ \u001b[31m =^.^= \u001b[0m ║ \u001b[34m       \u001b[0m ║ \u001b[32m<x)))><\u001b[0m ║\n",
      "╚═════════╩═════════╩═════════╝[0]\n",
      "╔═════════╦═════════╦═════════╗\n",
      "║ \u001b[31m =^.^= \u001b[0m ║ \u001b[34m       \u001b[0m ║ \u001b[32m<x)))><\u001b[0m ║\n",
      "╚═════════╩═════════╩═════════╝[1]\n",
      "╔═════════╦═════════╦═════════╗\n",
      "║ \u001b[31m =^.^= \u001b[0m ║ \u001b[34m       \u001b[0m ║ \u001b[32m<x)))><\u001b[0m ║\n",
      "╚═════════╩═════════╩═════════╝[2]\n",
      "╔═════════╦═════════╦═════════╗\n",
      "║ \u001b[31m =^.^= \u001b[0m ║ \u001b[34m       \u001b[0m ║ \u001b[32m<x)))><\u001b[0m ║\n",
      "╚═════════╩═════════╩═════════╝[3]\n",
      "╔═════════╦═════════╦═════════╗\n",
      "║ \u001b[31m =^.^= \u001b[0m ║ \u001b[34m       \u001b[0m ║ \u001b[32m<x)))><\u001b[0m ║\n",
      "╚═════════╩═════════╩═════════╝[4]\n",
      "╔═════════╦═════════╦═════════╗\n",
      "║ \u001b[31m =^.^= \u001b[0m ║ \u001b[34m       \u001b[0m ║ \u001b[32m<x)))><\u001b[0m ║\n",
      "╚═════════╩═════════╩═════════╝[5]\n",
      "╔═════════╦═════════╦═════════╗\n",
      "║ \u001b[31m =^.^= \u001b[0m ║ \u001b[34m       \u001b[0m ║ \u001b[32m<x)))><\u001b[0m ║\n",
      "╚═════════╩═════════╩═════════╝[6]\n",
      "╔═════════╦═════════╦═════════╗\n",
      "║ \u001b[31m =^.^= \u001b[0m ║ \u001b[34m       \u001b[0m ║ \u001b[32m<x)))><\u001b[0m ║\n",
      "╚═════════╩═════════╩═════════╝[7]\n",
      "╔═════════╦═════════╦═════════╗\n",
      "║ \u001b[31m =^.^= \u001b[0m ║ \u001b[34m       \u001b[0m ║ \u001b[32m<x)))><\u001b[0m ║\n",
      "╚═════════╩═════════╩═════════╝[8]\n",
      "╔═════════╦═════════╦═════════╗\n",
      "║ \u001b[34m       \u001b[0m ║ \u001b[31m =^.^= \u001b[0m ║ \u001b[32m<x)))><\u001b[0m ║\n",
      "╚═════════╩═════════╩═════════╝[9]\n",
      "╔═════════╦═════════╦═════════╗\n",
      "║ \u001b[34m       \u001b[0m ║ \u001b[34m       \u001b[0m ║ \u001b[31m =^.^= \u001b[0m ║\n",
      "╚═════════╩═════════╩═════════╝[10]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "random_policy = hb.Policy(mdp=mdp)\n",
    "episode = hb.Episode(mdp=mdp, policy=random_policy, max_depth=1000)\n",
    "episode.run(\"ascii\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    "\n",
    "The code below runs the policy iteration algorithm. The pseudocode can be found \n",
    "> insert citation url here or something idk yet\n",
    "\n",
    "Calling the algorithm only takes an MDP object and $\\theta$, this defines how precise the algorithm must update its values before terminating.\n",
    "\n",
    "The optimal policy and optimal state values of the MDP can be retrieved by calling the `run()` function on your `PolicyIteration` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 0.000_000_000_1\n",
    "\n",
    "policy_iteration = hb.PolicyIteration(mdp=mdp, theta_threshold=theta)\n",
    "\n",
    "# Run PolicyIteration\n",
    "pi_optimal_policy, pi_state_values, pi_q_values = policy_iteration.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying policy/state values\n",
    "\n",
    "The policy can be displayed by simply printing the `Policy` object.\n",
    "\n",
    "The learned state values can be displayed by calling `hb.Render.preview_V(mdp, state_values)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔═══════╦═══════╦═══════╗\n",
      "║  9.00 ║ 10.00 ║  0.00 ║\n",
      "╚═══════╩═══════╩═══════╝\n",
      "-----\n",
      "╔═════════╦═════════╦═════════╗\n",
      "║    →    ║    →    ║ ↑/↓/←/→ ║\n",
      "╚═════════╩═════════╩═════════╝\n",
      "-----\n",
      "{[0, 0]: {↑: 8.0, ↓: 8.0, ←: 8.0, →: 9.0}, [0, 1]: {↑: 9.0, ↓: 9.0, ←: 8.0, →: 10.0}, [0, 2]: {↑: 0, ↓: 0, ←: 0, →: 0}}\n"
     ]
    }
   ],
   "source": [
    "hb.Render.preview_V(mdp=mdp, learned_V=pi_state_values)\n",
    "\n",
    "print('-----')\n",
    "\n",
    "print(pi_optimal_policy)\n",
    "\n",
    "print('-----')\n",
    "\n",
    "print(pi_q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "The code below runs the value iteration algorithm, the pseudocode can be found \n",
    "> insert citation url here or something idk yet\n",
    "\n",
    "It works pretty much the same as the policy iteration algorithm, aside from the name of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 0.000_000_000_1\n",
    "\n",
    "value_iteration = hb.ValueIteration(mdp=mdp, theta_threshold=theta)\n",
    "\n",
    "vi_optimal_policy, vi_state_values, vi_q_values = value_iteration.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔═══════╦═══════╦═══════╗\n",
      "║  9.00 ║ 10.00 ║  0.00 ║\n",
      "╚═══════╩═══════╩═══════╝\n",
      "-----\n",
      "╔═════════╦═════════╦═════════╗\n",
      "║    →    ║    →    ║ ↑/↓/←/→ ║\n",
      "╚═════════╩═════════╩═════════╝\n",
      "-----\n",
      "{[0, 0]: {↑: 8.0, ↓: 8.0, ←: 8.0, →: 9.0}, [0, 1]: {↑: 9.0, ↓: 9.0, ←: 8.0, →: 10.0}, [0, 2]: {↑: 0, ↓: 0, ←: 0, →: 0}}\n"
     ]
    }
   ],
   "source": [
    "hb.Render.preview_V(mdp=mdp, learned_V=vi_state_values)\n",
    "\n",
    "print('-----')\n",
    "\n",
    "print(vi_optimal_policy)\n",
    "\n",
    "print('-----')\n",
    "\n",
    "print(vi_q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run Monte Carlo Prediction a `MonteCarloPredictor` object needs to be initialized. \n",
    "\n",
    "After that a policy and sample count can be given as parameters, to the `evaluate_policy` method.\n",
    "\n",
    "Finally, the value functions of this object can be retrieved with `mc_predictor.value_functions` and previewed with `Render.preview_V`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔═══════╦═══════╦═══════╗\n",
      "║ -1.00 ║  2.98 ║  0.00 ║\n",
      "╚═══════╩═══════╩═══════╝\n"
     ]
    }
   ],
   "source": [
    "N = 100000\n",
    "mc_predictor = hb.MonteCarloPredictor(mdp)\n",
    "mc_predictor.evaluate_policy(random_policy, n_samples=N)\n",
    "hb.Render.preview_V(mdp=mdp, learned_V=mc_predictor.value_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Control\n",
    "\n",
    "Monte Carlo control works in a similar fashion. First an opbject is initiliazed which then can be trained using the `.train` method.\n",
    "\n",
    "The optimal policy can be retrieved by calling `.policy` on the `MonteCarloController` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N = 100000\n",
    "mc_control = hb.MonteCarloController(mdp, epsilon=0.25)\n",
    "mc_control.train(n_episodes=N)\n",
    "trained_policy = mc_control.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This trained policy can be ran in an episode. the render mode `\"sar\"` can be used to give a quick overview of actions taken by the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔═════════╦═════════╦═════════╗\n",
      "║    →    ║    →    ║ ↑/↓/←/→ ║\n",
      "╚═════════╩═════════╩═════════╝\n",
      "t: 0 | S: [0, 1], R: -1, A: ↑\n",
      "t: 1 | S: [0, 1], R: -1, A: →\n",
      "t: 2 | S: [0, 2], R: 10, A: None\n"
     ]
    }
   ],
   "source": [
    "print(trained_policy)\n",
    "episode = hb.Episode(mdp=mdp, policy=trained_policy, max_depth=1000)\n",
    "episode.run(\"sar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
