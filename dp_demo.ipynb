{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All initialization tests passed.\n",
      "imported herringbone without any errors :)\n"
     ]
    }
   ],
   "source": [
    "import herringbone as hb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an MDP\n",
    "\n",
    "An MDP is formally defined as a 5-tuple $\\mathcal{M} = (S, A, P, R, \\gamma)$, where:\n",
    "- $S$ defines the state space\n",
    "- $A$ defines the action space\n",
    "- $P$ models the environment dynamics\n",
    "- $R$ models the reward function\n",
    "- $\\gamma$ defines the discount factor\n",
    "\n",
    "To create an MDP with this framework, it needs paths to at least a state config, a map, and an action config.\n",
    "\n",
    "Additionally, it can take an array of transition matrices (see $P$ in formal MDP definition), a seed, and the discount factor $\\gamma$. \n",
    "But these have default values, so do not fret if you do not understand them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_names = [\"slides\", \"example\", \"easy\", \"danger_holes\", \"double_fish\", \"wall_of_death\", \"example2\", \"mega\"]\n",
    "selected_map_id = 2\n",
    "\n",
    "state_path = \"herringbone/env_core/config/state_config.json\"\n",
    "map_path = f\"herringbone/env_core/maps/{map_names[selected_map_id]}.csv\"\n",
    "action_path = \"herringbone/env_core/config/action_config.json\"\n",
    "\n",
    "gamma = 1\n",
    "\n",
    "mdp = hb.MDP(state_path, map_path, action_path, seed=42, gamma=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previewing the board\n",
    "\n",
    "The board can be previewed with the following code.\n",
    "\n",
    "**Render Modes**\n",
    "1. `'sar'`: prints the state, action, reward of each iteration (only used in Monte Carlo simulations and Temporal Difference learning)\n",
    "2. `'rewards'`: prints the board with the calculated rewards for each state\n",
    "3. `'ascii'`: prints an ascii representation of the board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔═════════╦═════════╦═════════╗\n",
      "║ \u001b[34m       \u001b[0m ║ \u001b[34m       \u001b[0m ║ \u001b[32m<x)))><\u001b[0m ║\n",
      "╚═════════╩═════════╩═════════╝[0]\n"
     ]
    }
   ],
   "source": [
    "render_modes = ['sar', 'rewards', 'ascii']\n",
    "hb.Render.preview_frame(board=mdp.get_board(), agent_state=None, render_mode=render_modes[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    "\n",
    "The code below runs the policy iteration algorithm. The pseudocode can be found \n",
    "> insert citation url here or something idk yet\n",
    "\n",
    "Calling the algorithm only takes an MDP object and $\\theta$, this defines how precise the algorithm must update its values before terminating.\n",
    "\n",
    "The optimal policy and optimal state values of the MDP can be retrieved by calling the `run()` function on your `PolicyIteration` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 0.000_000_000_1\n",
    "\n",
    "policy_iteration = hb.PolicyIteration(mdp=mdp, theta_threshold=theta)\n",
    "\n",
    "# Run PolicyIteration\n",
    "pi_optimal_policy, pi_state_values, pi_q_values = policy_iteration.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying policy/state values\n",
    "\n",
    "The policy can be displayed by simply printing the `Policy` object.\n",
    "\n",
    "The learned state values can be displayed by calling `hb.Render.preview_V(mdp, state_values)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔═══════╦═══════╦═══════╗\n",
      "║  9.00 ║ 10.00 ║  0.00 ║\n",
      "╚═══════╩═══════╩═══════╝\n",
      "-----\n",
      "╔═════════╦═════════╦═════════╗\n",
      "║    →    ║    →    ║ ↑/↓/←/→ ║\n",
      "╚═════════╩═════════╩═════════╝\n",
      "-----\n",
      "{[0, 0]: {↑: 8.0, ↓: 8.0, ←: 8.0, →: 9.0}, [0, 1]: {↑: 9.0, ↓: 9.0, ←: 8.0, →: 10.0}, [0, 2]: {↑: 0, ↓: 0, ←: 0, →: 0}}\n"
     ]
    }
   ],
   "source": [
    "hb.Render.preview_V(mdp=mdp, learned_V=pi_state_values)\n",
    "\n",
    "print('-----')\n",
    "\n",
    "print(pi_optimal_policy)\n",
    "\n",
    "print('-----')\n",
    "\n",
    "print(pi_q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "The code below runs the value iteration algorithm, the pseudocode can be found \n",
    "> insert citation url here or something idk yet\n",
    "\n",
    "It works pretty much the same as the policy iteration algorithm, aside from the name of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 0.000_000_000_1\n",
    "\n",
    "value_iteration = hb.ValueIteration(mdp=mdp, theta_threshold=theta)\n",
    "\n",
    "vi_optimal_policy, vi_state_values, vi_q_values = value_iteration.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔═══════╦═══════╦═══════╗\n",
      "║  9.00 ║ 10.00 ║  0.00 ║\n",
      "╚═══════╩═══════╩═══════╝\n",
      "-----\n",
      "╔═════════╦═════════╦═════════╗\n",
      "║    →    ║    →    ║ ↑/↓/←/→ ║\n",
      "╚═════════╩═════════╩═════════╝\n",
      "-----\n",
      "{[0, 0]: {↑: 8.0, ↓: 8.0, ←: 8.0, →: 9.0}, [0, 1]: {↑: 9.0, ↓: 9.0, ←: 8.0, →: 10.0}, [0, 2]: {↑: 0, ↓: 0, ←: 0, →: 0}}\n"
     ]
    }
   ],
   "source": [
    "hb.Render.preview_V(mdp=mdp, learned_V=vi_state_values)\n",
    "\n",
    "print('-----')\n",
    "\n",
    "print(vi_optimal_policy)\n",
    "\n",
    "print('-----')\n",
    "\n",
    "print(vi_q_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
